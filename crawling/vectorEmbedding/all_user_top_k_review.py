import os
import json
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

import weaviate
from weaviate.auth import AuthApiKey
from weaviate.classes import query as wq


# ========== CONFIG ==========
CONFIG = {
    "USER_FILE": r"C:\Users\changjin\workspace\lab\pln\data_set\5_user_info.csv",
    "DATA_DIR": r"C:\Users\changjin\workspace\lab\pln\data_set\null_X",
    "OUTPUT_DIR": r"C:\Users\changjin\workspace\lab\pln\vectorEmbedding\user_results",
    "TOP_K": 30,
    "GAMMA": 0.3   # ë¦¬ë·°ìˆ˜ ê°€ì¤‘ì¹˜
}

CATEGORY_FILES = {
    "Accommodation": "accommodations_fixed.csv",
    "ì¹´í˜": "cafe_fixed.csv",
    "ìŒì‹ì ": "restaurants_fixed.csv",
    "ê´€ê´‘ì§€": "attractions_fixed.csv"
}

# ì¹´í…Œê³ ë¦¬ í•œê¸€ â†’ ì˜ì–´ ë³€í™˜ ë§¤í•‘
CATEGORY_TRANSLATE = {
    "Accommodation": "Accommodation",
    "ì¹´í˜": "Cafe",
    "ìŒì‹ì ": "Restaurant",
    "ê´€ê´‘ì§€": "Attraction"
}


# ========== 1. í™˜ê²½ ë³€ìˆ˜ ë° í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ==========
print("ğŸ” í™˜ê²½ ë³€ìˆ˜ ë¡œë”©...")
load_dotenv()

api_key = os.getenv("WEAVIATE_API_KEY")
cluster_url = os.getenv("WEAVIATE_CLUSTER_URL")

client = weaviate.connect_to_weaviate_cloud(
    cluster_url=cluster_url,
    auth_credentials=AuthApiKey(api_key)
)
print("âœ… Weaviate ì—°ê²° ì™„ë£Œ\n")

collection = client.collections.get("Place")


# ========== 2. ëª¨ë¸ ë¡œë“œ ==========
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")


# ========== 3. ì¶”ì²œ í•¨ìˆ˜ ==========
def rerank_with_penalty(user_like_vec, user_dislike_vecs, category_name,
                        top_k=30, alpha=1.0, beta=0.5, dislike_threshold=0.75):
    results = collection.query.near_vector(
        near_vector=user_like_vec.tolist(),
        limit=top_k*3,
        return_metadata=["distance"],
        include_vector=True,
        filters=wq.Filter.by_property("category").equal(category_name)
    )

    scored = []
    for obj in results.objects:
        like_sim = 1 - obj.metadata.distance

        place_dislike_vec = obj.properties.get("dislike_embedding", [])
        max_dislike_sim = 0
        if place_dislike_vec:
            sims = [
                cosine_similarity([ud], [place_dislike_vec])[0][0]
                for ud in user_dislike_vecs if len(ud) > 0
            ]
            max_dislike_sim = max(sims) if sims else 0

        if max_dislike_sim > dislike_threshold:
            continue

        sim_score = alpha * like_sim - beta * max_dislike_sim
        scored.append((obj, sim_score))

    scored.sort(key=lambda x: x[1], reverse=True)
    return scored[:top_k]


# ========== 4. ë¦¬ë·°ìˆ˜ ê¸°ë°˜ ì •ê·œí™” + ìµœì¢… ìŠ¤ì½”ì–´ ==========
def attach_review_scores_and_final(results_by_cat, data_dir, gamma=0.3):
    final_scores = {}

    for cat, scored_list in results_by_cat.items():
        if not scored_list:
            continue

        df = pd.read_csv(os.path.join(data_dir, CATEGORY_FILES[cat]))
        review_col = "review_count" if cat == "Accommodation" else "all_review_count"
        review_dict = dict(zip(df["id"], df[review_col]))

        enriched = []
        for obj, sim_score in scored_list:
            pid = obj.properties.get("place_id")
            rc = review_dict.get(pid, 0)
            enriched.append((pid, sim_score, rc))

        counts = np.array([rc for _, _, rc in enriched], dtype=float)
        if counts.sum() > 0:
            counts = np.log1p(counts)
            exp_counts = np.exp(counts - counts.max())
            review_norms = exp_counts / exp_counts.sum()
        else:
            review_norms = np.ones(len(enriched)) / len(enriched)

        cat_list = []
        for (pid, sim_score, _), rn in zip(enriched, review_norms):
            final_score = (1 - gamma) * sim_score + gamma * rn
            cat_list.append({
                "id": pid,
                "category": CATEGORY_TRANSLATE[cat],  # âœ… ì˜ì–´ ë³€í™˜
                "final_score": float(final_score)
            })

        cat_list = sorted(cat_list, key=lambda x: x["final_score"], reverse=True)

        # âœ… key ìì²´ë„ ì˜ì–´ë¡œ ë³€í™˜
        final_scores[CATEGORY_TRANSLATE[cat]] = cat_list

    return final_scores


# ========== 5. ëª¨ë“  ìœ ì € ì²˜ë¦¬ ==========
user_df = pd.read_csv(CONFIG["USER_FILE"])
os.makedirs(CONFIG["OUTPUT_DIR"], exist_ok=True)

for idx, user in user_df.iterrows():
    user_id = user["user_id"]
    like_keywords = eval(user["like_keywords"])
    dislike_keywords = eval(user["dislike_keywords"])

    print(f"\nğŸ‘¤ Processing User {idx+1}/{len(user_df)} â†’ {user_id}")
    print("   ğŸ‘ like:", like_keywords)
    print("   ğŸ‘ dislike:", dislike_keywords)

    user_like_vec = model.encode(" ".join(like_keywords), convert_to_numpy=True)
    user_dislike_vecs = [model.encode(kw, convert_to_numpy=True) for kw in dislike_keywords]

    results_by_cat = {}
    for cat in CATEGORY_FILES.keys():
        results_by_cat[cat] = rerank_with_penalty(user_like_vec, user_dislike_vecs,
                                                  cat, top_k=CONFIG["TOP_K"])

    review_scores_by_cat = attach_review_scores_and_final(results_by_cat,
                                                          CONFIG["DATA_DIR"],
                                                          gamma=CONFIG["GAMMA"])

    # ìœ ì €ë³„ ê²°ê³¼ ì €ì¥
    out_path = os.path.join(CONFIG["OUTPUT_DIR"], f"{user_id}_recommendations.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(review_scores_by_cat, f, ensure_ascii=False, indent=2)

    print(f"âœ… {user_id} ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {out_path}")


# ========== 6. ì—°ê²° ì¢…ë£Œ ==========
client.close()
print("\nğŸ”’ ì „ì²´ ìœ ì € ì²˜ë¦¬ ì™„ë£Œ & ì—°ê²° ì¢…ë£Œ")
