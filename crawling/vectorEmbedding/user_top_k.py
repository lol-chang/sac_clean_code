import os
import json
import pickle
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

import weaviate
from weaviate.auth import AuthApiKey
from weaviate.classes import query as wq


# ========== 1. í™˜ê²½ ë³€ìˆ˜ ë° í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ==========
print("ğŸ” í™˜ê²½ ë³€ìˆ˜ ë¡œë”©...")
load_dotenv()

api_key = os.getenv("WEAVIATE_API_KEY")
cluster_url = os.getenv("WEAVIATE_CLUSTER_URL")

client = weaviate.connect_to_weaviate_cloud(
    cluster_url=cluster_url,
    auth_credentials=AuthApiKey(api_key)
)
print("âœ… Weaviate ì—°ê²° ì™„ë£Œ\n")

collection = client.collections.get("Place")


# ========== 2. ëª¨ë¸ ë¡œë“œ ==========
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")


# ========== 3. ìœ ì € ë°ì´í„° ë¡œë“œ ==========
user_path = r"C:\Users\changjin\workspace\lab\pln\data_set\1_user_info.csv"
user_df = pd.read_csv(user_path)

user = user_df.iloc[0]  # ì²« ë²ˆì§¸ ìœ ì €ë§Œ í…ŒìŠ¤íŠ¸
like_keywords = eval(user["like_keywords"])  # ë¬¸ìì—´ -> ë¦¬ìŠ¤íŠ¸ ë³€í™˜
dislike_keywords = eval(user["dislike_keywords"])

print("ğŸ‘¤ User:", user["user_id"])
print("   ğŸ‘ like:", like_keywords)
print("   ğŸ‘ dislike:", dislike_keywords)

# ë²¡í„°í™”
user_like_vec = model.encode(" ".join(like_keywords), convert_to_numpy=True)
user_dislike_vecs = [model.encode(kw, convert_to_numpy=True) for kw in dislike_keywords]


# ========== 4. ì¶”ì²œ í•¨ìˆ˜ ==========
def rerank_with_penalty(category_name, top_k=30, alpha=1.0, beta=0.5, dislike_threshold=0.75):
    print(f"\nğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {category_name} (Top-{top_k})")

    # 1ì°¨ í›„ë³´êµ° (like ê¸°ë°˜)
    results = collection.query.near_vector(
        near_vector=user_like_vec.tolist(),
        limit=top_k*3,  # ì—¬ìœ ìˆê²Œ ë½‘ìŒ
        return_metadata=["distance"],
        include_vector=True,
        filters=wq.Filter.by_property("category").equal(category_name)
    )

    scored = []
    for obj in results.objects:
        like_sim = 1 - obj.metadata.distance  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„

        # dislike ë¹„êµ
        place_dislike_vec = obj.properties.get("dislike_embedding", [])
        max_dislike_sim = 0
        if place_dislike_vec:
            sims = [
                cosine_similarity([ud], [place_dislike_vec])[0][0]
                for ud in user_dislike_vecs if len(ud) > 0
            ]
            max_dislike_sim = max(sims) if sims else 0

        # í•˜ë“œ í•„í„°ë§
        if max_dislike_sim > dislike_threshold:
            continue

        # ì ìˆ˜ ê³„ì‚° (like - dislike)
        final_score = alpha * like_sim - beta * max_dislike_sim
        scored.append((obj, final_score, like_sim, max_dislike_sim))

    # ì ìˆ˜ìˆœ ì •ë ¬ í›„ Top-K
    scored.sort(key=lambda x: x[1], reverse=True)
    scored = scored[:top_k]

    return scored


# ========== 5. ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì²œ ==========
categories = ["Accommodation", "ì¹´í˜", "ìŒì‹ì ", "ê´€ê´‘ì§€"]
results_by_cat = {}

for cat in categories:
    results_by_cat[cat] = rerank_with_penalty(cat, top_k=30)


# ========== 6. ë¦¬ë·°ìˆ˜ ê¸°ë°˜ ì •ê·œí™” (ì¹´í…Œê³ ë¦¬ë³„) ==========
def attach_review_scores_by_category(results_by_cat, data_dir):
    fname_map = {
        "Accommodation": "accommodations_fixed.csv",
        "ì¹´í˜": "cafe_fixed.csv",
        "ìŒì‹ì ": "restaurants_fixed.csv",
        "ê´€ê´‘ì§€": "attractions_fixed.csv"
    }

    final_scores = {}

    for cat, scored_list in results_by_cat.items():
        if not scored_list:
            continue

        # âœ… CSV ë¡œë“œ
        df = pd.read_csv(os.path.join(data_dir, fname_map[cat]))
        review_col = "review_count" if cat == "Accommodation" else "all_review_count"
        review_dict = dict(zip(df["id"], df[review_col]))

        # âœ… review count ë¶™ì´ê¸°
        enriched = []
        for obj, _, _, _ in scored_list:
            pid = obj.properties.get("place_id")
            rc = review_dict.get(pid, 0)
            enriched.append((pid, rc))

        # âœ… log + softmax ê³„ì‚°
        counts = np.array([rc for _, rc in enriched], dtype=float)
        if counts.sum() > 0:
            counts = np.log1p(counts)  # ë¡œê·¸ ë³€í™˜
            exp_counts = np.exp(counts - counts.max())  # ì•ˆì •ì  softmax
            softmax_scores = exp_counts / exp_counts.sum()
        else:
            softmax_scores = np.ones_like(counts) / len(counts)

        # âœ… ì¹´í…Œê³ ë¦¬ë³„ ì €ì¥
        cat_list = []
        for (pid, _), s in zip(enriched, softmax_scores):
            cat_list.append({
                "id": pid,
                "review_norm": float(s)
            })

        # ì •ê·œí™” ê°’ ê¸°ì¤€ ì •ë ¬
        cat_list = sorted(cat_list, key=lambda x: x["review_norm"], reverse=True)
        final_scores[cat] = cat_list

    return final_scores


# ========== 7. ì‹¤í–‰ ==========
data_dir = r"C:\Users\changjin\workspace\lab\pln\data_set\null_X"
review_scores_by_cat = attach_review_scores_by_category(results_by_cat, data_dir)

print("âœ… ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ë·°ìˆ˜ ì •ê·œí™” + ì •ë ¬ ì™„ë£Œ")

# ì˜ˆì‹œ ì¶œë ¥
for cat, items in review_scores_by_cat.items():
    print(f"\nğŸ·ï¸ {cat}")
    for i, item in enumerate(items[:5], 1):
        print(f"  [{i}] id={item['id']}, norm={item['review_norm']:.4f}")


# ========== 8. ì—°ê²° ì¢…ë£Œ ==========
client.close()
print("\nğŸ”’ ì—°ê²° ì¢…ë£Œ ì™„ë£Œ")
